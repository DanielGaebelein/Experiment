# Recommendation System

<center>
    11712639 王冬青   
    11712903 侯润芃
</center>

#### 1. Multi-Behavior Recommendation System


##### 1.1 Single-Behavior Recommendation System

##### 1.1 Learn from Implicit Data
What we did last semester is to do recommendation learning from implicit data, which measures whether a user interacts with an item. It can be defined as the user–item interaction matrix Y ∈ $R^{M×N}$:
$$y_{ui} = \begin{cases}1, & if\ interaction\ (user\ u,\ item\ i)\ is\ observed;\\0, & otherwise\end{cases}$$
Here a value of 1 for $y_{ui}$ indicates that there is an interaction between user u and item i (like clicking, browsing, purchasing etc.); however, it does not mean u actually likes i. Similarly, a value of 0 does not necessarily mean u does not like i, it can be that the user is not aware of the item. 

In this case, training data can be extended with user's implicit feedback on items. It alleviates data sparsity, but is lack of behavior semantics.

##### 1.2 Learn from Multi-Behavior Data

The biggest feature of multi-behavior is to semantically divides the interaction between user and item into multiple user-item interaction matrices. Each of the behavior mentioned in 1.1, such as clicking, browsing and purchasing, has its own semantics and context. And there is a sequential relationship between different types of behavior, such as having to click before you can purchase. Learning recommendation from multi-behavior data can make better use of the semantics of behavior types. Similar with the representation of implicate data in 1.1, multi-behavior data can be defined as:$\{Y^1, Y^2, ..., Y^R\} $ ∈ $R^{M\times N}$:
$$
y_{ui}^r = \begin{cases}1, & if\ interaction\ (user\ u,\ item\ i)\ is\ observed\ under\ behavior\ r;\\0, & otherwise\end{cases}
$$
Here behavior types have a total order and sort them from the lowest level to the highest level: $Y^1 → Y^2... → Y^R $, where $Y^R$ denotes the target behavior,like the purchase in Taobao.

#### 2. Multi-Task Learning

Multi-task learning has been used successfully across all applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. MTL comes in many guises: joint learning, learning to learn, and learning with auxiliary tasks are only some names that have been used to refer to it. Generally, as soon as you find yourself optimizing more than one loss function, you are effectively doing multi-task learning (in contrast to single-task learning). In those scenarios, it helps to think about what you are trying to do explicitly in terms of MTL and to draw insights from it.

#### 3. Neural Collaborative Filtering

The model of NeuMF is as following:

![1578301143866](https://github.com/DanielGaebelein/Experiment/blob/master/pictures/1578301143866.png)

<center>Fig.</center>
- Input layer includes two feature vectors, which represent user u and item i in the form of one hot code. 
- Embedding layer, 4 latent factor matrices P_MF, P_MLP and Q_MF, Q_MLP are relative to the user and item vectors, MF and MLP respectively. 
- Neural CF Layers use User Latent vector and Item Latent vector generated by Embedding Layer as input.
- Output Layer, output the prediction score.



#### 4. Multi-Task Recommendation Based on NCF

![1585082870424](https://github.com/DanielGaebelein/Experiment/blob/master/pictures/1585082870424.png)





#### 5. Dataset



#### 6. Future Work



#### 7. Reference



(test:$$\frac{2^n}{3}$$)

