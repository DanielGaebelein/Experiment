# Recommendation System

<center>
    11712639 王冬青   
    11712903 侯润芃
</center>

#### 1. Multi-Behavior Recommendation System


##### 1.1 Learn from Implicit Data
Implicit feedback can indirectly reflect user preferences by referring to watching videos, buying products, and clicking on items. Compared to explicit feedback (i.e. rating and comments), implicit feedback can be tracked automatically, which makes it easier for content providers to collect. Using implicit feedback also allow us to get much more data than explicit feedback from users' behaviors.

What we did last semester is to do recommendation learning from implicit data, which measures whether a user interacts with an item. It can be defined as the user–item interaction matrix Y ∈ $R^{M×N}$:
$$y_{ui} = \begin{cases}1, & if\ interaction\ (user\ u,\ item\ i)\ is\ observed;\\0, & otherwise\end{cases}$$
Here a value of 1 for $y_{ui}$ indicates that there is an interaction between user u and item i (like clicking, browsing, purchasing etc.); however, it does not mean u actually likes i. Similarly, a value of 0 does not necessarily mean u does not like i, it can be that the user is not aware of the item. 

In this case, training data can be extended with user's implicit feedback on items. It alleviates data sparsity, but is lack of behavior semantics.

##### 1.2 Learn from Multi-Behavior Data

The biggest feature of multi-behavior is to semantically divides the interaction between user and item into multiple user-item interaction matrices. Each of the behavior mentioned in 1.1, such as clicking, browsing and purchasing, has its own semantics and context. And there is a sequential relationship between different types of behavior, such as having to click before you can purchase. Learning recommendation from multi-behavior data can make better use of the semantics of behavior types. Similar with the representation of implicate data in 1.1, multi-behavior data can be defined as:$\{Y^1, Y^2, ..., Y^R\} $ ∈ $R^{M\times N}$:
$$
y_{ui}^r = \begin{cases}1, & if\ interaction\ (user\ u,\ item\ i)\ is\ observed\ under\ behavior\ r;\\0, & otherwise\end{cases}
$$
Here behavior types have a total order and sort them from the lowest level to the highest level: $Y^1 → Y^2... → Y^R $, where $Y^R$ denotes the target behavior,like the purchase in Taobao. And the goal is to use the lower level behavior semantics to help target behavior get a better recommendation performance.


#### 2. Multi-Task Learning

Multi-task learning has been used successfully across all applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. MTL comes in many guises: joint learning, learning to learn, and learning with auxiliary tasks are only some names that have been used to refer to it. Generally, as soon as you find yourself optimizing more than one loss function, you are effectively doing multi-task learning (in contrast to single-task learning). In those scenarios, it helps to think about what you are trying to do explicitly in terms of MTL and to draw insights from it.


#### 3. Neural Collaborative Filtering

The basic model of multi-behavior recommendation refers to the NeuMF, which is an instance of the Neural Collaborative Filtering(NCF) model proposed by Dr. He xiangnan in 2017. NeuMF is a combination of GMF and MLP, both are instances of NCF. GMF uses element-wise product of vectors to simulate the potential feature interaction between users and items.MLP uses multiple neural network layers to learn interaction functions from the data. This model has been proved to have better recommendation performance than ItemPop, ItemKNN, BPR, eALS models. The model of NeuMF is as following:

![1578301143866](https://github.com/DanielGaebelein/Experiment/blob/master/pictures/1578301143866.png)

<center>Fig.</center>
- Input layer includes two feature vectors, which represent user u and item i in the form of one hot code. 
- Embedding layer, 4 latent factor matrices P_MF, P_MLP and Q_MF, Q_MLP are relative to the user and item vectors, MF and MLP respectively. 
- Neural CF Layers use User Latent vector and Item Latent vector generated by Embedding Layer as input.
- Output Layer, output the prediction score.


#### 4. Multi-Task Recommendation Based on NCF

![1585082870424](https://github.com/DanielGaebelein/Experiment/blob/master/pictures/1585082870424.png)

NCF can do end-to-end recommendation by using user's historical behavior data. Based on this, we apply NCF as a part of  our cross-domain recommendation system. And decided to improve it from two aspects: 

- using multi-task recommendation to optimize multi-domain behavior simultaneously;
-  extract rich semantic information from users' behaviors to increase the accuracy of recommendations.

The feature vectors of users and items in NCF architecture, which represent the potential characteristics of users and items, respectively constitute the embedded layer matrix of users and items for the input of MF and MLP. In last semester's experiment we migrated the feature matrix of the embedded layer of users in the source domain to the target domain, which improved the prediction accuracy and startup speed of the target domain. And it also demonstrated  that the potential feature vector does contain user behavior characteristics.

Since embedding layer represents the feature of human behavior，we can divide past target behavior into a sequence of  sub-behavior， which will lead to more accurate results. As we know, people always click, check and sometimes collect items before they buy on the Internet. In the past works, we just concerns about the final target behavior but ignore the middle behaviors. Inspired by Chen Gao,  user behavior can be divide into different levels in semantic order.

On the one hand, we can obtain more available data and mine more information contained in the data through such a hierarchical structure. On the other hand, we can do prediction on different behaviors to meet different needs. For example, the goal of Taobao's recommendation system is selling products, however movie sites may need to increase customer clicks. Therefore, when migrating across domains, we can improve the effect of cross-domain recommendation by migrating features of different levels .


On the one hand, we can obtain more available data and mine more information contained in the data through such a hierarchical structure. On the other hand, we can do prediction on different behaviors to meet different needs. For example, the goal of Taobao's recommendation system is selling products, however movie sites may need to increase customer clicks. Therefore, when migrating across domains, we can improve the effect of cross-domain recommendation by migrating features of different levels .


#### 5. Dataset

We did not find the dataset mentioned in paper [1], but we found a dataset with similar properties ----"UserBehavior". It is a dataset of Taobao users' behavior provided by Alibaba for research on implicit feedback and recommendation problems. And it contains behaviors (including  click, purchase, adding item to shopping cart and item favoring) of about one million random users with such behaviors between November 25, 2017 and December 3, 2017. Dimensions and Fields of the dataset are:

| Dimension         | Number      |
| ----------------- | ----------- |
| # of users        | 987,994     |
| # of items        | 4,162,024   |
| # of categories   | 9,439       |
| # of interactions | 100,150,807 |

| Field         | Explanation                                                  |
| ------------- | ------------------------------------------------------------ |
| User ID       | An integer, the serialized ID that represents a user         |
| Item ID       | An integer, the serialized ID that represents an item        |
| Category ID   | An integer, the serialized ID that represents the category which the corresponding item belongs to |
| Behavior type | A string, enum-type from ('pv'(page view), 'buy', 'cart'(add to shopping cart), 'fav'(favor an item)) |
| Timestamp     | An integer, the timestamp of the behavior                    |

Note: The scale of this dataset is too large, and we should reduce user-item matrix size (perhaps reduce to $10^4$*$10^4$) to do experiments. And as experience, running a user-item matrix of scale $10^3$ *$10^3​$ for one iteration in one NeuMF unit needs more than 90 seconds on our own computer, let alone several NeuMF units. Thus. it is necessary to have better devices to do experiments. We will turn to supervisor for help.


#### 6. Future Work



#### 7. Reference



(test:$$\frac{2^n}{3}$$)

